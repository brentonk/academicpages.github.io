---
title: "Artificially Precise Extremism: How Internet-Trained LLMs Exaggerate Our Differences"
collection: papers
permalink: /papers/llm-measurement
date: 2023-05-02
paperurl: "https://osf.io/preprints/socarxiv/5ecfa/"
coauthors: "Jim Bisbee, Josh Clinton, Cassy Dorff, and Jenn Larson"
excerpt: ""
rr: Political Analysis
---

**Abstract.**
Large Language Models (LLMs) offer new research possibilities for social scientists, but their potential as “synthetic data” is still largely unknown. In this note, we investigate the potential of using the popular closed-source LLM ChatGPT to measure human opinion. We show that although ChatGPT-generated opinions are similar to human opinion for some groups of US respondents, synthetic opinions also significantly exaggerate the extremity and certainty of partisan and social divisions. Responses from prompted “persona” profiles in ChatGPT produce measures of partisan and racial affective polarization that are seven times larger than the average opinion of humans who possess the same attributes as the prompted personas. Furthermore, synthetic data are artificially precise, with a standard deviation that is only 31% of the variation found in actual opinions among comparable humans. Because LLMs are proprietary, it is difficult to pinpoint the source of these biases, but our findings raise important questions about the appropriateness of replacing human opinion with synthetic responses generated by closed-source LLMs.
